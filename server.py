import warnings
import os
import re
import asyncio
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
from openai import OpenAI
from duckduckgo_search import DDGS
from fastapi.responses import HTMLResponse
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–∏—à–Ω–∏–µ –≤–æ—Ä–Ω–∏–Ω–≥–∏
warnings.filterwarnings("ignore")

app = FastAPI(title="Frantai Smart Orchestrator")

# --- 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø (API –ö–õ–Æ–ß–ò) ---
KEYS = {
    "cerebras": os.getenv("CEREBRAS_KEY", "csk-x989hhdnn9p2nexmt24ndk9ten4k3cmd82je9k4jxcnjwh6x"),
    "sambanova": os.getenv("SAMBANOVA_KEY", "6460e865-5a60-4cd8-b854-07d8d991b344"),
    "groq": os.getenv("GROQ_KEY", "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"),
    "openrouter": os.getenv("OPENROUTER_KEY", "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")
}

TG_TOKEN = os.getenv("TELEGRAM_TOKEN", "6796190792:AAEngQeCe0z7XtwhyqCpB-1ADWgBOXx9VWo")

FAMILIES = {
    "deepseek": {
        "heavy": [
            {"url": "https://api.sambanova.ai/v1", "key": KEYS["sambanova"], "model": "DeepSeek-R1", "heavy": True},
            {"url": "https://openrouter.ai/api/v1", "key": KEYS["openrouter"], "model": "deepseek/deepseek-r1:free", "heavy": True}
        ],
        "fast": [
            {"url": "https://api.groq.com/openai/v1", "key": KEYS["groq"], "model": "deepseek-r1-distill-llama-70b", "heavy": False}
        ]
    },
    "llama": {
        "heavy": [
            {"url": "https://api.sambanova.ai/v1", "key": KEYS["sambanova"], "model": "Meta-Llama-3.1-405B-Instruct", "heavy": True},
            {"url": "https://api.groq.com/openai/v1", "key": KEYS["groq"], "model": "llama-3.3-70b-versatile", "heavy": True}
        ],
        "fast": [
            {"url": "https://api.cerebras.ai/v1", "key": KEYS["cerebras"], "model": "llama-3.3-70b", "heavy": False}, 
            {"url": "https://api.groq.com/openai/v1", "key": KEYS["groq"], "model": "gemma2-9b-it", "heavy": False}
        ]
    }
}

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    family: str = "llama"
    thinking: bool = True
    intent: Optional[str] = None

# --- 2. –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–û–î–£–õ–ò (–ü–ï–†–ï–í–û–î, –ü–û–ò–°–ö, –ò–ù–¢–ï–ù–¢) ---

async def quick_translate(text: str, target_lang: str):
    """–ë–∞–∑–æ–≤—ã–π –ø–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ –±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å Groq"""
    try:
        client = OpenAI(api_key=KEYS["groq"], base_url="https://api.groq.com/openai/v1")
        prompt = f"Translate to {target_lang}. Output ONLY translation: {text}"
        resp = await asyncio.get_event_loop().run_in_executor(None, lambda: client.chat.completions.create(
            model="llama-3.1-8b-instant", messages=[{"role": "user", "content": prompt}], timeout=10
        ))
        return resp.choices[0].message.content.strip()
    except: return text

async def smart_translate_back(text: str):
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π, –ù–ï —Ç—Ä–æ–≥–∞—è –±–ª–æ–∫–∏ –∫–æ–¥–∞ ```...```"""
    if len(re.findall(r'[–∞-—è–ê-–Ø]', text)) > len(text) * 0.15:
        return text
        
    parts = re.split(r'(```[\s\S]*?```)', text)
    translated_parts = []
    for part in parts:
        if part.startswith('```'):
            translated_parts.append(part)
        elif part.strip():
            translated_parts.append(await quick_translate(part, "Russian"))
        else:
            translated_parts.append(part)
    return "".join(translated_parts)

async def analyze_intent(text: str) -> str:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        client = OpenAI(api_key=KEYS["groq"], base_url="[https://api.groq.com/openai/v1](https://api.groq.com/openai/v1)")
        messages = [
            {"role": "system", "content": "Categorize query: CODE, MATH, RESEARCH, GENERAL. Reply with ONE word."},
            {"role": "user", "content": text}
        ]
        resp = await asyncio.get_event_loop().run_in_executor(None, lambda: client.chat.completions.create(
            model="llama-3.1-8b-instant", messages=messages, temperature=0, timeout=7
        ))
        raw = resp.choices[0].message.content.strip().upper()
        match = re.search(r'(CODE|MATH|RESEARCH|GENERAL)', raw)
        return match.group(0) if match else "GENERAL"
    except: return "GENERAL"

async def perplexity_search(query: str) -> str:
    """–ü–æ–∏—Å–∫ –≤ —Å–µ—Ç–∏ —á–µ—Ä–µ–∑ DuckDuckGo"""
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, region='ru-ru', max_results=3))
            return "\n\n".join([f"Source: {r['title']}\n{r['body']}" for r in results])
    except: return ""

def analyze_complexity(text: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ Heavy/Fast –º–æ–¥–µ–ª–∏"""
    score = 0
    if len(text.split()) > 15: score += 2
    if re.search(r'[{}[\]()=;<>?]', text): score += 3
    return score >= 3

def get_family_by_intent(intent: str) -> str:
    return {"CODE": "deepseek", "MATH": "llama", "RESEARCH": "deepseek"}.get(intent, "llama")

# --- 3. –¶–ï–ù–¢–†–ê–õ–¨–ù–´–ô –ü–†–û–¶–ï–°–°–û–† (CORE AI) ---

@app.post("/ask")
async def ask_ai(request: ChatRequest):
    user_query = request.messages[-1].content
    intent = request.intent or await analyze_intent(user_query)
    
    if intent == "RESEARCH":
        context = await perplexity_search(user_query)
        if context: request.messages[-1].content += f"\n\nWEB CONTEXT:\n{context}"

    f_name = request.family.lower()
    f_data = FAMILIES.get(f_name, FAMILIES['llama'])
    is_complex = analyze_complexity(user_query)
    queue = f_data["heavy"] + f_data["fast"] if is_complex else f_data["fast"] + f_data["heavy"]
    is_russian = bool(re.search('[–∞-—è–ê-–Ø]', user_query))

    for target in queue:
        try:
            current_content = request.messages[-1].content
            if target["heavy"] and is_russian:
                current_content = await quick_translate(current_content, "English")
                current_content += "\n\nSTRICT: Answer in Russian, but keep ALL code and variables in English blocks."

            client = OpenAI(api_key=target['key'].strip(), base_url=target['url'])
            req_messages = [m.model_dump() for m in request.messages]
            req_messages[-1]["content"] = current_content

            resp = await asyncio.get_event_loop().run_in_executor(None, lambda: client.chat.completions.create(
                model=target['model'], messages=req_messages, timeout=85
            ))

            final_content = resp.choices[0].message.content
            
            if target["heavy"] and is_russian:
                final_content = await smart_translate_back(final_content)

            res = {"status": "success", "model": resp.model, "intent": intent, "content": final_content}
            if request.thinking and hasattr(resp.choices[0].message, 'reasoning_content'):
                res["thought"] = resp.choices[0].message.reasoning_content
            return res
        except: continue
    raise HTTPException(status_code=503, detail="Failover exhausted")

@app.post("/expert")
async def ask_expert(request: ChatRequest):
    intent = await analyze_intent(request.messages[-1].content)
    request.family = get_family_by_intent(intent)
    request.intent = intent
    return await ask_ai(request)

# --- 4. –¢–ï–õ–ï–ì–†–ê–ú –ë–û–¢ (–ò–ù–¢–ï–†–§–ï–ô–°) ---

if TG_TOKEN:
    bot = Bot(token=TG_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.MARKDOWN))
    dp = Dispatcher()

    @dp.message(Command("start"))
    async def start_h(m: types.Message):
        await m.answer("üöÄ *Frantai Smart Orchestrator Active*\n–ó–∞–¥–∞–≤–∞–π –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å.")

    @dp.message()
    async def message_handler(message: types.Message):
        await bot.send_chat_action(chat_id=message.chat.id, action="typing")
        
        try:
            req = ChatRequest(messages=[Message(role="user", content=message.text)])
            response = await ask_expert(req)
            
            content = response.get('content', '')
            thought = response.get('thought', '')
            header = f"ü§ñ *Model:* `{response['model']}`\nüéØ *Intent:* `{response['intent']}`\n\n"

            # –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            async def safe_send(text, prefix=""):
                if not text: return
                
                # –õ–∏–º–∏—Ç —á—É—Ç—å –º–µ–Ω—å—à–µ 4096 –¥–ª—è –∑–∞–ø–∞—Å–∞ –Ω–∞ —Ä–∞–∑–º–µ—Ç–∫—É
                limit = 3900 
                
                # –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏
                chunks = []
                while len(text) > 0:
                    if len(text) <= limit:
                        chunks.append(text)
                        break
                    split_pos = text.rfind('\n', 0, limit)
                    if split_pos <= 0: split_pos = limit
                    chunks.append(text[:split_pos])
                    text = text[split_pos:].lstrip()

                # –û—Ç–ø—Ä–∞–≤–∫–∞ –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏
                for i, chunk in enumerate(chunks):
                    msg_part = f"{prefix} (–ß–∞—Å—Ç—å {i+1})\n\n{chunk}" if len(chunks) > 1 else f"{prefix}\n\n{chunk}"
                    try:
                        await message.answer(msg_part, parse_mode=ParseMode.MARKDOWN)
                    except:
                        await message.answer(msg_part, parse_mode=None)

            # 1. –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –º—ã—Å–ª–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if thought:
                await safe_send(thought, prefix="üß† *–ú—ã—Å–ª–∏ –º–æ–¥–µ–ª–∏:*")

            # 2. –ó–∞—Ç–µ–º –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
            await safe_send(content, prefix=header)

        except Exception as e:
            # –í—ã–≤–æ–¥–∏–º –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—É—é –æ—à–∏–±–∫—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            print(f"Error in handler: {e}")
            await message.answer(f"‚ùå –û—à–∏–±–∫–∞: {str(e)[:100]}")
# --- 5. –ó–ê–ü–£–°–ö –ò –ó–î–û–†–û–í–¨–ï ---

@app.on_event("startup")
async def on_startup():
    if TG_TOKEN:
        await bot.delete_webhook(drop_pending_updates=True)
        asyncio.create_task(dp.start_polling(bot))

@app.get("/")
async def root(): return HTMLResponse("<h2>Orchestrator + TG Bot Online</h2>")

@app.get("/health")
async def health(): return {"status": "ok"}

if __name__ == "__main__":
    import uvicorn
    # Render –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è PORT
    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("PORT", 8000)))